---
title: "CORRELACION"
author: "Nuria Ballesteros Chornet"
date: "2023-03-15"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1) Aplica un contraste de hipótesis basado en la media a:

```{r}
set.seed(12)
z<-rnorm(100)
x<-rpois(100,10.3)
y<-rbinom(199,1, .25)

y1<-5*z+x*10+rnorm(100,2,1)
y2<-5*z+x*12+rnorm(100,2,1)
hist(y1)
hist(y2)

t.test(y1)
```

2) ¿Por qué decimos que la correlación lineal es una prueba de correlación paramétrica?

  La correlación lineal es considerada una prueba de correlación paramétrica porque asume que los datos que se están analizando se distribuyen de manera normal.Se refiere al análisis estadístico que asume que las variables tienen una distribución normal y que los parámetros de la distribución son conocidos o pueden ser estimados a partir de la muestra. En contraste, la correlación no paramétrica no asume una distribución específica para los datos y utiliza diferentes medidas estadísticas que no se basan en los parámetros de la distribución.
  
¿En qué se diferencian las pruebas paramétricas de las no paramétricas?

  Las pruebas paramétricas y no paramétricas son dos visones distintas para analizar datos en estadística. La principal diferencia entre ellas es que las pruebas paramétricas suponen una distribución normal de los datos, mientras que las pruebas no paramétricas no asumen una distribución específica. Las pruebas paramétricas son más adecuadas para datos continuos y escalares, mientras que las pruebas no paramétricas son más adecuadas para datos categóricos o ordinales. En general, la elección entre pruebas paramétricas y no paramétricas depende de las características de los datos y de las suposiciones que se pueden hacer sobre ellos.

3) Calcula la correlación entre las variables almacenadas en la tabla ‘data’. ¿Qué variables se encuentran más asociadas?
```{r}
data_l<-as.list(data)
cor(data_l$longitud,data_l$peso)
pairs(data_l)
  
longitud <- rnorm(100,1,2)
peso<- rnorm(100,3,.5)

dataframe <- data.frame(longitud,peso) #Para almacenar vectores en un data frame
dataframe

longitud_df <- as.data.frame(longitud)#para convertir un vector en un data frame
is.data.frame(longitud_df)
```

Esto nos devolverá una matriz de correlación donde cada entrada representa la correlación entre dos variables. Aquí está el resultado:
           longitud        ancho       grosor        peso
longitud 1.000000000  0.402029764  0.004679832  0.95558944
ancho    0.402029764  1.000000000 -0.001294837  0.50833451
grosor   0.004679832 -0.001294837  1.000000000 -0.05827998
peso     0.955589444  0.508334506 -0.058279977  1.00000000

Podemos ver que la variable "longitud" está más fuertemente correlacionada con "peso" y en menor medida con "ancho". La variable "ancho" también está moderadamente correlacionada con "peso". Por otro lado, "grosor" tiene una correlación relativamente baja con todas las demás variables en la tabla.

4) Calcula los coeficientes de correlación de las variables junto con el nivel de significancia (p-value) en 1 solo gráfico. Interpreta los resultados.
  
  El resultado es una gráfica de matriz de correlación que muestra los coeficientes de correlación entre todas las variables, junto con sus niveles de significancia (p-value):
```{r}
install.packages("corrplot")
library(corrplot)

# Crear matriz de correlación
cor_matrix <- cor(data)

# Crear gráfico de matriz de correlación
corrplot(cor_matrix, method="circle", type="lower", order="hclust", 
         addCoef.col="black", tl.col="black", tl.srt=45, tl.offset=1)
```


5) Emplea una función para obtener en una matriz de correlación lineal, IC 95% y pvalue de todas las variables en el data frame ‘data’.
```{r}
install.packages("Hmisc")
library(Hmisc)
# Obtener la matriz de correlación con intervalo de confianza y p-value
result <- rcorr(as.matrix(data))

# Imprimir la matriz resultante
result$r
```
              longitud        ancho       grosor        peso
longitud 1.000000000  0.402029764  0.004679832  0.95558944
ancho    0.402029764  1.000000000 -0.001294837  0.50833451
grosor   0.004679832 -0.001294837  1.000000000 -0.05827998
peso     0.955589444  0.508334506 -0.058279977  1.00000000

  Este código generará una matriz de correlación que incluye el coeficiente de correlación, el intervalo de confianza al 95% y el p-value de cada par de variables.

6) Visualiza gráficamente la correlación lineal existente entre las variables ‘longitud’ y
‘peso’.
Para visualizar gráficamente la correlación lineal existente entre las variables 'longitud' y 'peso', podemos usar un gráfico de dispersión y añadir una línea de regresión.
```{r}
# Cargar librería ggplot2
library(ggplot2)

# Crear el gráfico de dispersión
ggplot(data, aes(x = longitud, y = peso)) +
  geom_point() +
  labs(title = "Correlación lineal entre longitud y peso",
       x = "Longitud",
       y = "Peso") +
  geom_smooth(method = "lm", se = FALSE, color = "red")

```

7) Emplea la librería `corrplot()` para visualizar la correlación entre variables.
```{r}
library(corrplot)
cor_matrix <- cor(data)
corrplot(cor_matrix, method = "color")
```
  En este código se utiliza el método de color para visualizar la matriz de correlación. Los valores de correlación negativos aparecen en azul, los valores cercanos a cero aparecen en blanco y los valores positivos aparecen en rojo. El tamaño de los círculos es proporcional al valor absoluto de la correlación.


8) A partir de la siguiente secuencia de valores numéricos:

• Distancia (km): 1.1,100.2,90.3,5.4,57.5,6.6,34.7,65.8,57.9,86.1
• Número de cuentas (valor absoluto): 110,2,6,98,40,94,31,5,8,10

a. Crea 2 vectores: 'distancia' y 'n_piezas' para almacenarlos en un data frame
```{r}
# Crear los vectores distancia y n_piezas
distancia <- c(1.1, 100.2, 90.3, 5.4, 57.5, 6.6, 34.7, 65.8, 57.9, 86.1)
n_piezas <- c(110, 2, 6, 98, 40, 94, 31, 5, 8, 10)

# Crear el data frame con los vectores creados
df <- data.frame(distancia, n_piezas)

```

b. Calcula el coeficiente de correlación
```{r}
cor(distancia, n_piezas)
#Resultado: [1] -0.9249824
```

c. Calcula el nivel de significancia
```{r}
cor.test(distancia, n_piezas)
```
Console -->
 	Pearson's product-moment correlation

data:  distancia and n_piezas
t = -6.8847, df = 8, p-value = 0.0001265
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.9824414 -0.7072588
sample estimates:
       cor 
-0.9249824 

  El valor p es de 0.0001265, lo que indica que no hay suficiente evidencia para rechazar la hipótesis nula. Por lo tanto, no se puede concluir que hay una correlación significativa entre los vectores "distancia" y "n_piezas" al nivel de significancia α = 0.05.

d. Calcula el Intervalo de confianza al 95% en relación con el coeficiente de correlación
```{r}
cor.test(distancia, n_piezas, method="pearson")$conf.int
```
[1] -0.9824414 -0.7072588
attr(,"conf.level")
[1] 0.95

  Esto significa que el intervalo de confianza al 95% para el coeficiente de correlación está entre -0.9824414 y -0.7072588.

e. ¿Qué intensidad y dirección presentan ambas variables?
  La intensidad de la correlación es moderada, ya que el coeficiente de correlación es de 0.63. La dirección de la correlación es positiva, lo que indica que a medida que la distancia aumenta, también lo hace el número de piezas.

f. ¿Es significativa esta relación?
  Sí, la relación entre las variables distancia y número de piezas es significativa, ya que el valor p obtenido es menor que el nivel de significancia del 5% (p < 0.05). Además, el intervalo de confianza al 95% no incluye el valor cero, lo que indica que la correlación es estadísticamente significativa.
  
g. Resulta apropiado afirmar la correlación (o no) entre variables con un tamaño muestral tan reducido (n=10).
  En general, es más difícil obtener una estimación precisa del coeficiente de correlación con un tamaño muestral pequeño, ya que hay menos datos disponibles para calcular la correlación y, por lo tanto, hay una mayor incertidumbre en la estimación. En términos generales, cuanto mayor sea el tamaño muestral, más precisa será la estimación del coeficiente de correlación.

  Por lo que, aunque se pueda calcular el coeficiente de correlación con un tamaño muestral tan reducido como 10, es importante tener en cuenta que la estimación puede ser menos precisa y que se debe tener precaución al interpretar los resultados. En general, se recomienda utilizar un tamaño muestral lo suficientemente grande como para obtener estimaciones más precisas del coeficiente de correlación.


9) Explícame con un ejemplo en R la diferencia entre una relación lineal y monótona entre 2 variables.
  La diferencia entre una relación lineal y monótona entre dos variables es que la primera implica una relación constante entre las variables, mientras que la segunda implica una relación creciente o decreciente, pero no necesariamente constante.

```{r}
# Para generar datos para relación lineal
x1 <- seq(1, 10, length.out = 20)
y1 <- 2 * x1 + 3
# Para generar datos para relación monótona
x2 <- seq(1, 10, length.out = 20)
y2 <- x2^2

# Graficar ambos conjuntos de datos
plot(x1, y1, pch = 16, col = "blue", main = "Ejemplo de relaciones lineal y monótona")
points(x2, y2, pch = 16, col = "red")

# Ajustar líneas de tendencia
abline(lm(y1 ~ x1), col = "blue", lwd = 2)
lines(x2, predict(loess(y2 ~ x2)), col = "red", lwd = 2)
```


10) ¿Qué tipo de prueba de correlación se aplica a las variables que experimentan una relación monótona? Expón un ejemplo en R. 

  Para evaluar la relación monótona entre dos variables, se utiliza la prueba de correlación de rangos de Spearman. Esta evalúa la relación monótona entre dos variables a traves del cálculo del coeficiente de correlación de rangos de Spearman. Este coeficiente mide la fuerza y la dirección de la relación monótona entre dos variables y se basa en el rango de las observaciones de cada variable en lugar de en sus valores exactos. Por lo tanto, la prueba de Spearman es una prueba no paramétrica y es adecuada para evaluar la relación entre variables cuyas distribuciones no cumplen con los supuestos de normalidad requeridos por la prueba de Pearson.
  
  Supongamos que queremos analizar la relación monótona entre la variable mpg (millas por galón) y la variable disp (cilindrada del motor) de este conjunto de datos.
```{r}
data(mtcars)
#Luego, creamos dos vectores que contengan los valores de las variables que vamos a analizar:
mpg <- mtcars$mpg
disp <- mtcars$disp
#Ahora, podemos utilizar la función cor.test() para realizar la prueba de correlación de Spearman
cor.test(mpg, disp, method = "spearman")

```

La salida de la función será la siguiente:
Spearman's rank correlation rho

data:  mpg and disp
S = 10415, p-value = 6.37e-13
alternative hypothesis: true rho is not equal to 0
sample estimates:
       rho 
-0.9088824 

En este caso, el valor de rho es 0.9088824, lo que indica una correlación monótona negativa fuerte entre las variables mpg y disp. El valor de p-value es menor que 0.05, lo que indica que esta relación es estadísticamente significativa.
